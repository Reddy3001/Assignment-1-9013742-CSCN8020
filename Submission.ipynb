{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07bab7af",
   "metadata": {},
   "source": [
    "# CSCN8020 – Assignment 1 \n",
    "\n",
    "**Course:** Reinforcement Learning Programming (CSCN8020)  \n",
    "**Student:** Jahnavi Pakanati\n",
    "**Student ID:** 9013742\n",
    "\n",
    "This notebook contains:\n",
    "- **Problem 1**: MDP design for a pick-and-place robot  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcdd4aee",
   "metadata": {},
   "source": [
    "## Problem 1 – Pick-and-Place Robot (MDP Design)\n",
    "\n",
    "We model the **pick-and-place** task as a **Markov Decision Process (MDP)**.\n",
    "\n",
    "#### 1) States (S)\n",
    "A state should capture the **minimum information** needed to choose good actions:\n",
    "- Robot arm pose (discretized or continuous), e.g., `(x, y, z)` or `(joint1, joint2, joint3, ...)`\n",
    "- Gripper: `open` / `closed`\n",
    "- Object status: `on_table` / `in_gripper` / `at_target`\n",
    "- (Optional) Velocities for smoothness: `vx, vy, vz`\n",
    "\n",
    "##### 2) Actions (A)\n",
    "Primitive, low-level actions that the policy can choose:\n",
    "- Arm motion: `move_up`, `move_down`, `move_left`, `move_right` (optionally `move_forward`, `move_backward`)\n",
    "- Gripper: `open_gripper`, `close_gripper`\n",
    "\n",
    "##### 3) Rewards (R)\n",
    "Shape the behavior to be **fast** and **smooth**:\n",
    "- `+10` when the object is placed at the target location (`at_target`)\n",
    "- `-1` per time step to discourage unnecessary movement\n",
    "- `-5` penalty for dropping the object or collisions\n",
    "\n",
    "> ***Reasoning:*** This reward encourages successful completion, penalizes wasted motion, and discourages unsafe behaviors. The state and action choices reflect the ***control levers*** the agent has and the **task status** it must track.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242551d1",
   "metadata": {},
   "source": [
    "# Problem 2 — 2×2 Gridworld (Two Value-Iteration Sweeps)\n",
    "\n",
    "**Setup**\n",
    "- **States:** \\(s_1=(0,0),\\ s_2=(0,1),\\ s_3=(1,0),\\ s_4=(1,1)\\)  \n",
    "- **Actions:** up, down, left, right  \n",
    "- **Transitions:** deterministic if valid; **invalid moves keep you in the same state** (\\(s' = s\\))  \n",
    "- **Rewards (per state):** \\(R(s_1)=5,\\ R(s_2)=10,\\ R(s_3)=1,\\ R(s_4)=2\\)  \n",
    "- **Discount:** \\(\\gamma=0.9\\)  \n",
    "- **Update rule (Bellman optimality for this state-reward setting):**  \n",
    "  \\[\n",
    "  V_{k+1}(s)\\ \\leftarrow\\ R(s)\\ +\\ \\gamma\\ \\max_{a\\in A}\\ V_k\\!\\big(s'(s,a)\\big)\n",
    "  \\]\n",
    "\n",
    "---\n",
    "\n",
    "## Iteration 0 (Initialization)\n",
    "All zeros:\n",
    "| State | \\(V_0(s)\\) |\n",
    "|---|---|\n",
    "| \\(s_1\\) | 0 |\n",
    "| \\(s_2\\) | 0 |\n",
    "| \\(s_3\\) | 0 |\n",
    "| \\(s_4\\) | 0 |\n",
    "\n",
    "---\n",
    "\n",
    "## Iteration 1\n",
    "Neighbors were 0 in Iteration 0, so:\n",
    "\\[\n",
    "V_1(s) = R(s) + \\gamma\\cdot 0 = R(s)\n",
    "\\]\n",
    "\n",
    "| State | Calculation | \\(V_1(s)\\) |\n",
    "|---|---|---|\n",
    "| \\(s_1\\) | \\(5 + 0.9\\cdot 0\\) | **5** |\n",
    "| \\(s_2\\) | \\(10 + 0.9\\cdot 0\\) | **10** |\n",
    "| \\(s_3\\) | \\(1 + 0.9\\cdot 0\\) | **1** |\n",
    "| \\(s_4\\) | \\(2 + 0.9\\cdot 0\\) | **2** |\n",
    "\n",
    "---\n",
    "\n",
    "## Iteration 2\n",
    "Look one step ahead using \\(V_1\\). *(Invalid moves allow “stay”.)*\n",
    "\n",
    "- **\\(s_1\\)**: up→\\(s_1\\) (5), left→\\(s_1\\) (5), right→\\(s_2\\) (10), down→\\(s_3\\) (1)  \n",
    "  \\[\n",
    "  V_2(s_1)=5 + 0.9\\cdot \\max(5,5,10,1)=5+0.9\\cdot 10=\\mathbf{14}\n",
    "  \\]\n",
    "- **\\(s_2\\)**: up→\\(s_2\\) (10), right→\\(s_2\\) (10), left→\\(s_1\\) (5), down→\\(s_4\\) (2)  \n",
    "  \\[\n",
    "  V_2(s_2)=10 + 0.9\\cdot \\max(10,10,5,2)=10+0.9\\cdot 10=\\mathbf{19}\n",
    "  \\]\n",
    "- **\\(s_3\\)**: left→\\(s_3\\) (1), down→\\(s_3\\) (1), up→\\(s_1\\) (5), right→\\(s_4\\) (2)  \n",
    "  \\[\n",
    "  V_2(s_3)=1 + 0.9\\cdot \\max(1,1,5,2)=1+0.9\\cdot 5=\\mathbf{5.5}\n",
    "  \\]\n",
    "- **\\(s_4\\)**: right→\\(s_4\\) (2), down→\\(s_4\\) (2), up→\\(s_2\\) (10), left→\\(s_3\\) (1)  \n",
    "  \\[\n",
    "  V_2(s_4)=2 + 0.9\\cdot \\max(2,2,10,1)=2+0.9\\cdot 10=\\mathbf{11}\n",
    "  \\]\n",
    "\n",
    "**Values after Iteration 2**\n",
    "\n",
    "| State | \\(V_2(s)\\) |\n",
    "|---|---|\n",
    "| \\(s_1\\) | **14.0** |\n",
    "| \\(s_2\\) | **19.0** |\n",
    "| \\(s_3\\) | **5.5** |\n",
    "| \\(s_4\\) | **11.0** |\n",
    "\n",
    "---\n",
    "\n",
    "## Greedy Policy after Iteration 2\n",
    "Pick \\(a=\\arg\\max_a \\big(R(s)+\\gamma V_2(s'(s,a))\\big)\\).\n",
    "\n",
    "- \\(s_1\\): best next is \\(s_2\\) → **right**  \n",
    "- \\(s_2\\): best is to **stay at \\(s_2\\)** (via an invalid move like up/right)  \n",
    "- \\(s_3\\): best next is \\(s_1\\) → **up**  \n",
    "- \\(s_4\\): best next is \\(s_2\\) → **up**\n",
    "\n",
    "> **Note:** If exploiting invalid moves to “stay” is **not allowed**, the best *valid* action from \\(s_2\\) is **left to \\(s_1\\)** (since \\(V_2(s_1)=14\\) > \\(V_2(s_4)=11\\)).\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e860fbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Standard VI: V* (converged in 9 sweeps, 0.0018s)\n",
      " -0.434   0.629   1.810   3.122   4.580\n",
      "  0.629   1.810   3.122   4.580   6.200\n",
      "  1.810   3.122   4.580   6.200   8.000\n",
      "  3.122   4.580   6.200   8.000  10.000\n",
      "  4.580   6.200   8.000  10.000   0.000\n",
      "\n",
      "Standard VI: π*\n",
      "↓ ↓ ↓ ↓ X\n",
      "↓ ↓ → ↓ ↓\n",
      "→ ↓ X ↓ ↓\n",
      "X ↓ ↓ ↓ ↓\n",
      "→ → → → •\n",
      "\n",
      "In-Place VI: V* (converged in 9 sweeps, 0.0013s)\n",
      " -0.434   0.629   1.810   3.122   4.580\n",
      "  0.629   1.810   3.122   4.580   6.200\n",
      "  1.810   3.122   4.580   6.200   8.000\n",
      "  3.122   4.580   6.200   8.000  10.000\n",
      "  4.580   6.200   8.000  10.000   0.000\n",
      "\n",
      "In-Place VI: π*\n",
      "↓ ↓ ↓ ↓ X\n",
      "↓ ↓ → ↓ ↓\n",
      "→ ↓ X ↓ ↓\n",
      "X ↓ ↓ ↓ ↓\n",
      "→ → → → •\n",
      "\n",
      "=== Convergence / Equality Check ===\n",
      "Standard VI sweeps: 9, time: 0.0018s\n",
      "In-Place  VI sweeps: 9, time: 0.0013s\n",
      "L1 difference between V* (should be ~0): 0.00000000\n",
      "OK: V* from both methods match within tolerance.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Problem 3: 5×5 Gridworld (Value Iteration + In-Place Variant)\n",
    "# ================================\n",
    "# Specs (from problem statement):\n",
    "#   • Goal (terminal): s_goal = (4,4) with reward +10 (episode ends after entering goal)\n",
    "#   • Grey states: {(2,2), (3,0), (0,4)} with reward −5\n",
    "#   • All other states: −1\n",
    "#   • Actions: up, down, left, right (deterministic). Invalid moves keep you in place.\n",
    "#   • Deliverables: value iteration (standard + in-place), V*, π*, comparison (iterations/time)\n",
    "#\n",
    "# Notes:\n",
    "#   • Reward is applied on ARRIVAL to the next state s′ (common “state reward” convention).\n",
    "#   • Terminal is absorbing: after you reach GOAL, future rewards are 0 and the state does not change.\n",
    "\n",
    "from pprint import pprint\n",
    "from time import perf_counter\n",
    "\n",
    "# ----- Environment definition -----\n",
    "H, W = 5, 5\n",
    "GOAL = (4, 4)\n",
    "GREY = {(2, 2), (3, 0), (0, 4)}\n",
    "ACTIONS = {\n",
    "    \"up\":    (-1,  0),\n",
    "    \"down\":  ( 1,  0),\n",
    "    \"left\":  ( 0, -1),\n",
    "    \"right\": ( 0,  1),\n",
    "}\n",
    "gamma = 0.9  # discount factor (can adjust if needed)\n",
    "\n",
    "def in_bounds(r, c, H=H, W=W):\n",
    "    return 0 <= r < H and 0 <= c < W\n",
    "\n",
    "def is_terminal(s):\n",
    "    return s == GOAL\n",
    "\n",
    "def reward(s):\n",
    "    \"\"\"Reward for being in state s (applied on arrival).\"\"\"\n",
    "    if s == GOAL:\n",
    "        return +10\n",
    "    if s in GREY:\n",
    "        return -5\n",
    "    return -1\n",
    "\n",
    "def step(s, a):\n",
    "    \"\"\"Deterministic transition; invalid moves keep you in place.\n",
    "       Returns (s_next, r), where r is reward on ARRIVAL to s_next.\n",
    "       Terminal is absorbing: no further reward after reaching GOAL.\"\"\"\n",
    "    if is_terminal(s):\n",
    "        return s, 0.0  # absorbing\n",
    "    r, c = s\n",
    "    dr, dc = ACTIONS[a]\n",
    "    nr, nc = r + dr, c + dc\n",
    "    if in_bounds(nr, nc):\n",
    "        s2 = (nr, nc)\n",
    "    else:\n",
    "        s2 = s  # bump into wall -> stay\n",
    "    return s2, reward(s2)\n",
    "\n",
    "STATES = [(r, c) for r in range(H) for c in range(W)]\n",
    "\n",
    "# ----- Value Iteration (Standard/Synchronous two-array) -----\n",
    "def value_iteration_synchronous(theta=1e-8, max_iters=10000):\n",
    "    \"\"\"Standard VI with a copy. Stops when max change < theta.\"\"\"\n",
    "    V = {s: 0.0 for s in STATES}  # often V(goal)=0; reward is earned upon entering goal\n",
    "    iters = 0\n",
    "    while True:\n",
    "        delta = 0.0\n",
    "        V_new = V.copy()\n",
    "        for s in STATES:\n",
    "            if is_terminal(s):\n",
    "                V_new[s] = 0.0  # terminal value (no future)\n",
    "                continue\n",
    "            best = float(\"-inf\")\n",
    "            for a in ACTIONS:\n",
    "                s2, r = step(s, a)\n",
    "                q = r + gamma * V[s2]\n",
    "                if q > best:\n",
    "                    best = q\n",
    "            delta = max(delta, abs(best - V[s]))\n",
    "            V_new[s] = best\n",
    "        V = V_new\n",
    "        iters += 1\n",
    "        if delta < theta or iters >= max_iters:\n",
    "            break\n",
    "    return V, iters\n",
    "\n",
    "# ----- Value Iteration (In-Place/Gauss–Seidel) -----\n",
    "def value_iteration_inplace(theta=1e-8, max_iters=10000):\n",
    "    \"\"\"In-place VI: reuses updated values within the same sweep. Typically converges faster.\"\"\"\n",
    "    V = {s: 0.0 for s in STATES}\n",
    "    iters = 0\n",
    "    while True:\n",
    "        delta = 0.0\n",
    "        for s in STATES:\n",
    "            if is_terminal(s):\n",
    "                V[s] = 0.0\n",
    "                continue\n",
    "            best = float(\"-inf\")\n",
    "            for a in ACTIONS:\n",
    "                s2, r = step(s, a)\n",
    "                q = r + gamma * V[s2]  # reuse newly updated V[s2] when available\n",
    "                if q > best:\n",
    "                    best = q\n",
    "            delta = max(delta, abs(best - V[s]))\n",
    "            V[s] = best\n",
    "        iters += 1\n",
    "        if delta < theta or iters >= max_iters:\n",
    "            break\n",
    "    return V, iters\n",
    "\n",
    "# ----- Greedy Policy from V -----\n",
    "def greedy_policy(V):\n",
    "    \"\"\"π*(s) = argmax_a [ r(s→s′) + γ V(s′) ]\"\"\"\n",
    "    pi = {}\n",
    "    for s in STATES:\n",
    "        if is_terminal(s):\n",
    "            pi[s] = \"•\"  # terminal marker\n",
    "            continue\n",
    "        best_a, best_q = None, float(\"-inf\")\n",
    "        for a in ACTIONS:\n",
    "            s2, r = step(s, a)\n",
    "            q = r + gamma * V[s2]\n",
    "            if q > best_q:\n",
    "                best_q, best_a = q, a\n",
    "        pi[s] = best_a\n",
    "    return pi\n",
    "\n",
    "# ----- Pretty printing helpers -----\n",
    "def print_value_grid(V, title=\"Value Function\"):\n",
    "    print(f\"\\n{title}\")\n",
    "    for r in range(H):\n",
    "        row = [f\"{V[(r,c)]:7.3f}\" for c in range(W)]\n",
    "        print(\" \".join(row))\n",
    "\n",
    "def print_policy_grid(pi, title=\"Policy (greedy)\"):\n",
    "    arrows = {\"up\":\"↑\", \"down\":\"↓\", \"left\":\"←\", \"right\":\"→\", \"•\":\"•\"}\n",
    "    print(f\"\\n{title}\")\n",
    "    for r in range(H):\n",
    "        row = []\n",
    "        for c in range(W):\n",
    "            s = (r, c)\n",
    "            if s in GREY and s != GOAL:\n",
    "                row.append(\"X\")  # mark grey cells\n",
    "            else:\n",
    "                row.append(arrows.get(pi[s], \"?\"))\n",
    "        print(\" \".join(row))\n",
    "\n",
    "def l1_diff(Va, Vb):\n",
    "    return sum(abs(Va[s] - Vb[s]) for s in STATES)\n",
    "\n",
    "# ----- Run both methods and compare -----\n",
    "t0 = perf_counter()\n",
    "V_sync, it_sync = value_iteration_synchronous(theta=1e-8, max_iters=10000)\n",
    "t1 = perf_counter()\n",
    "\n",
    "V_inp, it_inp = value_iteration_inplace(theta=1e-8, max_iters=10000)\n",
    "t2 = perf_counter()\n",
    "\n",
    "pi_sync = greedy_policy(V_sync)\n",
    "pi_inp  = greedy_policy(V_inp)\n",
    "\n",
    "print_value_grid(V_sync, title=f\"Standard VI: V* (converged in {it_sync} sweeps, {t1 - t0:.4f}s)\")\n",
    "print_policy_grid(pi_sync, title=\"Standard VI: π*\")\n",
    "\n",
    "print_value_grid(V_inp, title=f\"In-Place VI: V* (converged in {it_inp} sweeps, {t2 - t1:.4f}s)\")\n",
    "print_policy_grid(pi_inp, title=\"In-Place VI: π*\")\n",
    "\n",
    "print(\"\\n=== Convergence / Equality Check ===\")\n",
    "print(f\"Standard VI sweeps: {it_sync}, time: {t1 - t0:.4f}s\")\n",
    "print(f\"In-Place  VI sweeps: {it_inp}, time: {t2 - t1:.4f}s\")\n",
    "print(f\"L1 difference between V* (should be ~0): {l1_diff(V_sync, V_inp):.8f}\")\n",
    "\n",
    "# Optional: assert near-equality of value functions\n",
    "# (use a tolerance since floating point can differ slightly)\n",
    "tol = 1e-6\n",
    "if l1_diff(V_sync, V_inp) < tol:\n",
    "    print(\"OK: V* from both methods match within tolerance.\")\n",
    "else:\n",
    "    print(\"Warning: V* mismatch beyond tolerance.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89835bf4",
   "metadata": {},
   "source": [
    "- 5×5 grid: goal gives **+10**, grey cells **−5**, all other steps **−1**.  \n",
    "- Value Iteration repeatedly sets \\(V(s)=\\max_a\\{\\text{reward(next)}+\\gamma V(\\text{next})\\}\\), making squares nearer the goal worth more and penalizing grey/long routes.  \n",
    "- The greedy policy then points along the **shortest safe path** to the goal.  \n",
    "- **In-place** VI converges faster but ends with the **same \\(V^*\\) and \\(\\pi^*\\)** as standard VI.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5afc844a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Episode 5000/20000] updates so far: 1572\n",
      "[Episode 10000/20000] updates so far: 5956\n",
      "[Episode 15000/20000] updates so far: 10635\n",
      "[Episode 20000/20000] updates so far: 15537\n",
      "\n",
      "MC Off-Policy (IS): Estimated V(s)\n",
      " -1.360  -1.318  -1.851   3.067   4.084\n",
      " -1.212  -1.450  -1.327   4.542   6.166\n",
      " -1.791  -0.902   4.549   6.190   7.985\n",
      "  3.082   4.546   6.172   7.990  10.000\n",
      "  1.795   3.109   4.555   3.083   0.000\n",
      "\n",
      "MC Off-Policy (IS): Greedy π(s)\n",
      "↓ ↑ ← ↓ X\n",
      "← → ← ↓ ↓\n",
      "↑ → X ↓ ↓\n",
      "X → → → ↓\n",
      "→ → ↑ ← •\n",
      "\n",
      "=== Stats ===\n",
      "Episodes processed: 20000\n",
      "Total weighted updates: 15537\n",
      "Episodes with non-zero IS contribution (tail matched π): 20000\n",
      "Elapsed time: 5.543s\n",
      "\n",
      "Note: With a deterministic greedy target policy, only the tail of an episode that happens to follow the greedy action contributes (others break).\n",
      "Increase episodes for smoother V(s), or switch to per-decision IS / epsilon-greedy target for more coverage if allowed.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Gridworld: same 5×5 environment as Problem 3\n",
    "# ================================\n",
    "# Goal: Estimate the value function using episodes generated by a fixed behavior policy (random),\n",
    "#       while the target policy is greedy (off-policy MC control with importance sampling).\n",
    "#\n",
    "# Method: Sutton & Barto (MC Off-Policy Control, Weighted IS)\n",
    "#   - Behavior policy b(a|s): uniform over actions (0.25 each)\n",
    "#   - Target policy  π(a|s): greedy w.r.t. current Q(s,a) (deterministic)\n",
    "#   - Backward pass per episode with cumulative weight W\n",
    "#   - If a_t != π(s_t), break (since π(a_t|s_t)=0 → weight becomes 0)\n",
    "#\n",
    "# Outputs:\n",
    "#   • Estimated V(s) (from Q) and greedy π(s)\n",
    "#   • Basic stats on episodes/updates\n",
    "#   • (Optional) comparison with Value Iteration from Problem 3 (same V* expected qualitatively)\n",
    "# ================================\n",
    "\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from time import perf_counter\n",
    "\n",
    "# ----- Environment definition (same as Problem 3) -----\n",
    "H, W = 5, 5\n",
    "GOAL = (4, 4)\n",
    "GREY = {(2, 2), (3, 0), (0, 4)}\n",
    "ACTIONS = (\"up\", \"down\", \"left\", \"right\")\n",
    "DELTA = {\n",
    "    \"up\":    (-1,  0),\n",
    "    \"down\":  ( 1,  0),\n",
    "    \"left\":  ( 0, -1),\n",
    "    \"right\": ( 0,  1),\n",
    "}\n",
    "gamma = 0.9\n",
    "\n",
    "def in_bounds(r, c):\n",
    "    return 0 <= r < H and 0 <= c < W\n",
    "\n",
    "def is_terminal(s):\n",
    "    return s == GOAL\n",
    "\n",
    "def reward(s):\n",
    "    \"\"\"Reward on arrival to s.\"\"\"\n",
    "    if s == GOAL:\n",
    "        return +10\n",
    "    if s in GREY:\n",
    "        return -5\n",
    "    return -1\n",
    "\n",
    "def step(s, a):\n",
    "    \"\"\"Deterministic step; invalid moves keep you in place. Terminal is absorbing.\"\"\"\n",
    "    if is_terminal(s):\n",
    "        return s, 0.0\n",
    "    r, c = s\n",
    "    dr, dc = DELTA[a]\n",
    "    nr, nc = r + dr, c + dc\n",
    "    s2 = (nr, nc) if in_bounds(nr, nc) else s\n",
    "    return s2, reward(s2)\n",
    "\n",
    "STATES = [(r, c) for r in range(H) for c in range(W)]\n",
    "\n",
    "# ----- Behavior policy b(a|s): uniform random over 4 actions -----\n",
    "def b_sample_action(s):\n",
    "    return random.choice(ACTIONS)\n",
    "\n",
    "def b_prob(a, s):\n",
    "    return 1.0 / len(ACTIONS)  # = 0.25\n",
    "\n",
    "# ----- Target policy π(a|s): greedy w.r.t. Q(s,a) -----\n",
    "def greedy_action(Q, s):\n",
    "    \"\"\"Return argmax_a Q[s,a]. If ties, pick first; if unseen, default to 'up'.\"\"\"\n",
    "    if is_terminal(s):\n",
    "        return None\n",
    "    best_a, best_q = None, float(\"-inf\")\n",
    "    for a in ACTIONS:\n",
    "        q = Q[(s, a)]\n",
    "        if q > best_q:\n",
    "            best_q, best_a = q, a\n",
    "    return best_a\n",
    "\n",
    "# ----- Generate one episode with behavior policy b -----\n",
    "def generate_episode_b(max_steps=200):\n",
    "    \"\"\"Returns list of (s_t, a_t, r_{t+1}). Start from a random non-terminal state.\"\"\"\n",
    "    # Exploring starts: pick any non-terminal uniformly\n",
    "    s = random.choice([s for s in STATES if not is_terminal(s)])\n",
    "    episode = []\n",
    "    steps = 0\n",
    "    while not is_terminal(s) and steps < max_steps:\n",
    "        a = b_sample_action(s)\n",
    "        s2, r = step(s, a)\n",
    "        episode.append((s, a, r))\n",
    "        s = s2\n",
    "        steps += 1\n",
    "    return episode\n",
    "\n",
    "# ----- Off-Policy MC Control with (Weighted) Importance Sampling -----\n",
    "def offpolicy_mc_control(\n",
    "    num_episodes=20000,\n",
    "    max_steps_per_ep=200,\n",
    "    seed=42,\n",
    "    verbose_every=5000\n",
    "):\n",
    "    random.seed(seed)\n",
    "    # Action-value estimates and cumulative weights\n",
    "    Q = defaultdict(float)     # Q[(s,a)]\n",
    "    C = defaultdict(float)     # C[(s,a)] = sum of weights\n",
    "\n",
    "    updates = 0\n",
    "    nonzero_weight_episodes = 0\n",
    "    t_start = perf_counter()\n",
    "\n",
    "    for ep in range(1, num_episodes + 1):\n",
    "        episode = generate_episode_b(max_steps=max_steps_per_ep)\n",
    "        # Compute return G backward\n",
    "        G = 0.0\n",
    "        W = 1.0\n",
    "\n",
    "        # Backward pass\n",
    "        for t in reversed(range(len(episode))):\n",
    "            s_t, a_t, r_tp1 = episode[t]\n",
    "            G = gamma * G + r_tp1\n",
    "\n",
    "            # Current greedy action under target policy π\n",
    "            a_star = greedy_action(Q, s_t)\n",
    "            # If terminal, a_star is None; but we never store terminal steps as s_t (we stop before stepping from terminal)\n",
    "\n",
    "            # If behavior deviates from target action at this time, break (π(a_t|s_t)=0)\n",
    "            if a_star is not None and a_t != a_star:\n",
    "                break\n",
    "\n",
    "            # Weighted incremental update (Weighted Importance Sampling):\n",
    "            C[(s_t, a_t)] += W\n",
    "            # Q <- Q + (W/C) * (G - Q)\n",
    "            Q[(s_t, a_t)] += (W / C[(s_t, a_t)]) * (G - Q[(s_t, a_t)])\n",
    "            updates += 1\n",
    "\n",
    "            # If a_star is None, we are effectively at terminal; no further weighting\n",
    "            if a_star is None:\n",
    "                break\n",
    "\n",
    "            # Update weight: multiply by 1 / b(a_t|s_t) (since π(a_t|s_t)=1 when a_t == a_star)\n",
    "            W *= 1.0 / b_prob(a_t, s_t)\n",
    "\n",
    "        if W > 0:\n",
    "            nonzero_weight_episodes += 1\n",
    "\n",
    "        if verbose_every and ep % verbose_every == 0:\n",
    "            print(f\"[Episode {ep}/{num_episodes}] updates so far: {updates}\")\n",
    "\n",
    "    elapsed = perf_counter() - t_start\n",
    "    return Q, updates, nonzero_weight_episodes, elapsed\n",
    "\n",
    "# ----- Helpers to derive V and π from Q, and to pretty-print -----\n",
    "def V_from_Q(Q):\n",
    "    V = {}\n",
    "    for s in STATES:\n",
    "        if is_terminal(s):\n",
    "            V[s] = 0.0\n",
    "            continue\n",
    "        V[s] = max(Q[(s, a)] for a in ACTIONS)\n",
    "    return V\n",
    "\n",
    "def pi_from_Q(Q):\n",
    "    pi = {}\n",
    "    for s in STATES:\n",
    "        if is_terminal(s):\n",
    "            pi[s] = \"•\"\n",
    "        else:\n",
    "            pi[s] = greedy_action(Q, s)\n",
    "    return pi\n",
    "\n",
    "def print_value_grid(V, title=\"Value Function (MC Off-Policy)\"):\n",
    "    print(f\"\\n{title}\")\n",
    "    for r in range(H):\n",
    "        row = [f\"{V[(r,c)]:7.3f}\" for c in range(W)]\n",
    "        print(\" \".join(row))\n",
    "\n",
    "def print_policy_grid(pi, title=\"Policy (greedy from Q)\"):\n",
    "    arrows = {\"up\":\"↑\", \"down\":\"↓\", \"left\":\"←\", \"right\":\"→\", \"•\":\"•\"}\n",
    "    print(f\"\\n{title}\")\n",
    "    for r in range(H):\n",
    "        row = []\n",
    "        for c in range(W):\n",
    "            s = (r, c)\n",
    "            if s in GREY and s != GOAL:\n",
    "                row.append(\"X\")\n",
    "            else:\n",
    "                row.append(arrows.get(pi[s], \"?\"))\n",
    "        print(\" \".join(row))\n",
    "\n",
    "# ===== Run Off-Policy MC Control =====\n",
    "if __name__ == \"__main__\":\n",
    "    Q, updates, nz_eps, elapsed = offpolicy_mc_control(\n",
    "        num_episodes=20000,   # increase for smoother results (e.g., 50_000+)\n",
    "        max_steps_per_ep=200,\n",
    "        seed=7,\n",
    "        verbose_every=5000\n",
    "    )\n",
    "    V = V_from_Q(Q)\n",
    "    pi = pi_from_Q(Q)\n",
    "\n",
    "    print_value_grid(V, \"MC Off-Policy (IS): Estimated V(s)\")\n",
    "    print_policy_grid(pi, \"MC Off-Policy (IS): Greedy π(s)\")\n",
    "\n",
    "    print(\"\\n=== Stats ===\")\n",
    "    print(f\"Episodes processed: 20000\")\n",
    "    print(f\"Total weighted updates: {updates}\")\n",
    "    print(f\"Episodes with non-zero IS contribution (tail matched π): {nz_eps}\")\n",
    "    print(f\"Elapsed time: {elapsed:.3f}s\")\n",
    "\n",
    "    # (Optional) Quick qualitative note:\n",
    "    print(\"\\nNote: With a deterministic greedy target policy, only the tail of an episode that happens to follow the greedy action contributes (others break).\")\n",
    "    print(\"Increase episodes for smoother V(s), or switch to per-decision IS / epsilon-greedy target for more coverage if allowed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e794d1e4",
   "metadata": {},
   "source": [
    "- Generate episodes with a **random behavior policy**; the **target policy** is greedy.  \n",
    "- For each episode, compute return \\(G\\) and weight steps by **importance ratios** \\(\\pi(a|s)/b(a|s)\\); if an action isn’t the greedy one, the weight becomes 0 and we stop.  \n",
    "- Apply the weighted updates to estimate \\(Q(s,a)\\), then set \\(V(s)=\\max_a Q(s,a)\\) and the policy to **greedy from \\(Q\\)**.  \n",
    "- This is **model-free** (no transition model needed); more episodes ⇒ smoother values, unlike DP which uses known dynamics.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
