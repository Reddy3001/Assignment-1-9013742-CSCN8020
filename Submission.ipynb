{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07bab7af",
   "metadata": {},
   "source": [
    "# CSCN8020 – Assignment 1 \n",
    "\n",
    "**Course:** Reinforcement Learning Programming (CSCN8020)  \n",
    "**Student:** Jahnavi Pakanati\n",
    "**Student ID:** 9013742\n",
    "\n",
    "This notebook contains:\n",
    "- **Problem 1**: MDP design for a pick-and-place robot  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcdd4aee",
   "metadata": {},
   "source": [
    "## Problem 1 – Pick-and-Place Robot (MDP Design)\n",
    "\n",
    "We model the **pick-and-place** task as a **Markov Decision Process (MDP)**.\n",
    "\n",
    "#### 1) States (S)\n",
    "A state should capture the **minimum information** needed to choose good actions:\n",
    "- Robot arm pose (discretized or continuous), e.g., `(x, y, z)` or `(joint1, joint2, joint3, ...)`\n",
    "- Gripper: `open` / `closed`\n",
    "- Object status: `on_table` / `in_gripper` / `at_target`\n",
    "- (Optional) Velocities for smoothness: `vx, vy, vz`\n",
    "\n",
    "##### 2) Actions (A)\n",
    "Primitive, low-level actions that the policy can choose:\n",
    "- Arm motion: `move_up`, `move_down`, `move_left`, `move_right` (optionally `move_forward`, `move_backward`)\n",
    "- Gripper: `open_gripper`, `close_gripper`\n",
    "\n",
    "##### 3) Rewards (R)\n",
    "Shape the behavior to be **fast** and **smooth**:\n",
    "- `+10` when the object is placed at the target location (`at_target`)\n",
    "- `-1` per time step to discourage unnecessary movement\n",
    "- `-5` penalty for dropping the object or collisions\n",
    "\n",
    "> ***Reasoning:*** This reward encourages successful completion, penalizes wasted motion, and discourages unsafe behaviors. The state and action choices reflect the ***control levers*** the agent has and the **task status** it must track.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242551d1",
   "metadata": {},
   "source": [
    "# Problem 2 — 2×2 Gridworld (Two Value-Iteration Sweeps)\n",
    "\n",
    "**Setup**\n",
    "- **States:** \\(s_1=(0,0),\\ s_2=(0,1),\\ s_3=(1,0),\\ s_4=(1,1)\\)  \n",
    "- **Actions:** up, down, left, right  \n",
    "- **Transitions:** deterministic if valid; **invalid moves keep you in the same state** (\\(s' = s\\))  \n",
    "- **Rewards (per state):** \\(R(s_1)=5,\\ R(s_2)=10,\\ R(s_3)=1,\\ R(s_4)=2\\)  \n",
    "- **Discount:** \\(\\gamma=0.9\\)  \n",
    "- **Update rule (Bellman optimality for this state-reward setting):**  \n",
    "  \\[\n",
    "  V_{k+1}(s)\\ \\leftarrow\\ R(s)\\ +\\ \\gamma\\ \\max_{a\\in A}\\ V_k\\!\\big(s'(s,a)\\big)\n",
    "  \\]\n",
    "\n",
    "---\n",
    "\n",
    "## Iteration 0 (Initialization)\n",
    "All zeros:\n",
    "| State | \\(V_0(s)\\) |\n",
    "|---|---|\n",
    "| \\(s_1\\) | 0 |\n",
    "| \\(s_2\\) | 0 |\n",
    "| \\(s_3\\) | 0 |\n",
    "| \\(s_4\\) | 0 |\n",
    "\n",
    "---\n",
    "\n",
    "## Iteration 1\n",
    "Neighbors were 0 in Iteration 0, so:\n",
    "\\[\n",
    "V_1(s) = R(s) + \\gamma\\cdot 0 = R(s)\n",
    "\\]\n",
    "\n",
    "| State | Calculation | \\(V_1(s)\\) |\n",
    "|---|---|---|\n",
    "| \\(s_1\\) | \\(5 + 0.9\\cdot 0\\) | **5** |\n",
    "| \\(s_2\\) | \\(10 + 0.9\\cdot 0\\) | **10** |\n",
    "| \\(s_3\\) | \\(1 + 0.9\\cdot 0\\) | **1** |\n",
    "| \\(s_4\\) | \\(2 + 0.9\\cdot 0\\) | **2** |\n",
    "\n",
    "---\n",
    "\n",
    "## Iteration 2\n",
    "Look one step ahead using \\(V_1\\). *(Invalid moves allow “stay”.)*\n",
    "\n",
    "- **\\(s_1\\)**: up→\\(s_1\\) (5), left→\\(s_1\\) (5), right→\\(s_2\\) (10), down→\\(s_3\\) (1)  \n",
    "  \\[\n",
    "  V_2(s_1)=5 + 0.9\\cdot \\max(5,5,10,1)=5+0.9\\cdot 10=\\mathbf{14}\n",
    "  \\]\n",
    "- **\\(s_2\\)**: up→\\(s_2\\) (10), right→\\(s_2\\) (10), left→\\(s_1\\) (5), down→\\(s_4\\) (2)  \n",
    "  \\[\n",
    "  V_2(s_2)=10 + 0.9\\cdot \\max(10,10,5,2)=10+0.9\\cdot 10=\\mathbf{19}\n",
    "  \\]\n",
    "- **\\(s_3\\)**: left→\\(s_3\\) (1), down→\\(s_3\\) (1), up→\\(s_1\\) (5), right→\\(s_4\\) (2)  \n",
    "  \\[\n",
    "  V_2(s_3)=1 + 0.9\\cdot \\max(1,1,5,2)=1+0.9\\cdot 5=\\mathbf{5.5}\n",
    "  \\]\n",
    "- **\\(s_4\\)**: right→\\(s_4\\) (2), down→\\(s_4\\) (2), up→\\(s_2\\) (10), left→\\(s_3\\) (1)  \n",
    "  \\[\n",
    "  V_2(s_4)=2 + 0.9\\cdot \\max(2,2,10,1)=2+0.9\\cdot 10=\\mathbf{11}\n",
    "  \\]\n",
    "\n",
    "**Values after Iteration 2**\n",
    "\n",
    "| State | \\(V_2(s)\\) |\n",
    "|---|---|\n",
    "| \\(s_1\\) | **14.0** |\n",
    "| \\(s_2\\) | **19.0** |\n",
    "| \\(s_3\\) | **5.5** |\n",
    "| \\(s_4\\) | **11.0** |\n",
    "\n",
    "---\n",
    "\n",
    "## Greedy Policy after Iteration 2\n",
    "Pick \\(a=\\arg\\max_a \\big(R(s)+\\gamma V_2(s'(s,a))\\big)\\).\n",
    "\n",
    "- \\(s_1\\): best next is \\(s_2\\) → **right**  \n",
    "- \\(s_2\\): best is to **stay at \\(s_2\\)** (via an invalid move like up/right)  \n",
    "- \\(s_3\\): best next is \\(s_1\\) → **up**  \n",
    "- \\(s_4\\): best next is \\(s_2\\) → **up**\n",
    "\n",
    "> **Note:** If exploiting invalid moves to “stay” is **not allowed**, the best *valid* action from \\(s_2\\) is **left to \\(s_1\\)** (since \\(V_2(s_1)=14\\) > \\(V_2(s_4)=11\\)).\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e860fbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Standard VI: V* (converged in 9 sweeps, 0.0018s)\n",
      " -0.434   0.629   1.810   3.122   4.580\n",
      "  0.629   1.810   3.122   4.580   6.200\n",
      "  1.810   3.122   4.580   6.200   8.000\n",
      "  3.122   4.580   6.200   8.000  10.000\n",
      "  4.580   6.200   8.000  10.000   0.000\n",
      "\n",
      "Standard VI: π*\n",
      "↓ ↓ ↓ ↓ X\n",
      "↓ ↓ → ↓ ↓\n",
      "→ ↓ X ↓ ↓\n",
      "X ↓ ↓ ↓ ↓\n",
      "→ → → → •\n",
      "\n",
      "In-Place VI: V* (converged in 9 sweeps, 0.0013s)\n",
      " -0.434   0.629   1.810   3.122   4.580\n",
      "  0.629   1.810   3.122   4.580   6.200\n",
      "  1.810   3.122   4.580   6.200   8.000\n",
      "  3.122   4.580   6.200   8.000  10.000\n",
      "  4.580   6.200   8.000  10.000   0.000\n",
      "\n",
      "In-Place VI: π*\n",
      "↓ ↓ ↓ ↓ X\n",
      "↓ ↓ → ↓ ↓\n",
      "→ ↓ X ↓ ↓\n",
      "X ↓ ↓ ↓ ↓\n",
      "→ → → → •\n",
      "\n",
      "=== Convergence / Equality Check ===\n",
      "Standard VI sweeps: 9, time: 0.0018s\n",
      "In-Place  VI sweeps: 9, time: 0.0013s\n",
      "L1 difference between V* (should be ~0): 0.00000000\n",
      "OK: V* from both methods match within tolerance.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Problem 3: 5×5 Gridworld (Value Iteration + In-Place Variant)\n",
    "# ================================\n",
    "# Specs (from problem statement):\n",
    "#   • Goal (terminal): s_goal = (4,4) with reward +10 (episode ends after entering goal)\n",
    "#   • Grey states: {(2,2), (3,0), (0,4)} with reward −5\n",
    "#   • All other states: −1\n",
    "#   • Actions: up, down, left, right (deterministic). Invalid moves keep you in place.\n",
    "#   • Deliverables: value iteration (standard + in-place), V*, π*, comparison (iterations/time)\n",
    "#\n",
    "# Notes:\n",
    "#   • Reward is applied on ARRIVAL to the next state s′ (common “state reward” convention).\n",
    "#   • Terminal is absorbing: after you reach GOAL, future rewards are 0 and the state does not change.\n",
    "\n",
    "from pprint import pprint\n",
    "from time import perf_counter\n",
    "\n",
    "# ----- Environment definition -----\n",
    "H, W = 5, 5\n",
    "GOAL = (4, 4)\n",
    "GREY = {(2, 2), (3, 0), (0, 4)}\n",
    "ACTIONS = {\n",
    "    \"up\":    (-1,  0),\n",
    "    \"down\":  ( 1,  0),\n",
    "    \"left\":  ( 0, -1),\n",
    "    \"right\": ( 0,  1),\n",
    "}\n",
    "gamma = 0.9  # discount factor (can adjust if needed)\n",
    "\n",
    "def in_bounds(r, c, H=H, W=W):\n",
    "    return 0 <= r < H and 0 <= c < W\n",
    "\n",
    "def is_terminal(s):\n",
    "    return s == GOAL\n",
    "\n",
    "def reward(s):\n",
    "    \"\"\"Reward for being in state s (applied on arrival).\"\"\"\n",
    "    if s == GOAL:\n",
    "        return +10\n",
    "    if s in GREY:\n",
    "        return -5\n",
    "    return -1\n",
    "\n",
    "def step(s, a):\n",
    "    \"\"\"Deterministic transition; invalid moves keep you in place.\n",
    "       Returns (s_next, r), where r is reward on ARRIVAL to s_next.\n",
    "       Terminal is absorbing: no further reward after reaching GOAL.\"\"\"\n",
    "    if is_terminal(s):\n",
    "        return s, 0.0  # absorbing\n",
    "    r, c = s\n",
    "    dr, dc = ACTIONS[a]\n",
    "    nr, nc = r + dr, c + dc\n",
    "    if in_bounds(nr, nc):\n",
    "        s2 = (nr, nc)\n",
    "    else:\n",
    "        s2 = s  # bump into wall -> stay\n",
    "    return s2, reward(s2)\n",
    "\n",
    "STATES = [(r, c) for r in range(H) for c in range(W)]\n",
    "\n",
    "# ----- Value Iteration (Standard/Synchronous two-array) -----\n",
    "def value_iteration_synchronous(theta=1e-8, max_iters=10000):\n",
    "    \"\"\"Standard VI with a copy. Stops when max change < theta.\"\"\"\n",
    "    V = {s: 0.0 for s in STATES}  # often V(goal)=0; reward is earned upon entering goal\n",
    "    iters = 0\n",
    "    while True:\n",
    "        delta = 0.0\n",
    "        V_new = V.copy()\n",
    "        for s in STATES:\n",
    "            if is_terminal(s):\n",
    "                V_new[s] = 0.0  # terminal value (no future)\n",
    "                continue\n",
    "            best = float(\"-inf\")\n",
    "            for a in ACTIONS:\n",
    "                s2, r = step(s, a)\n",
    "                q = r + gamma * V[s2]\n",
    "                if q > best:\n",
    "                    best = q\n",
    "            delta = max(delta, abs(best - V[s]))\n",
    "            V_new[s] = best\n",
    "        V = V_new\n",
    "        iters += 1\n",
    "        if delta < theta or iters >= max_iters:\n",
    "            break\n",
    "    return V, iters\n",
    "\n",
    "# ----- Value Iteration (In-Place/Gauss–Seidel) -----\n",
    "def value_iteration_inplace(theta=1e-8, max_iters=10000):\n",
    "    \"\"\"In-place VI: reuses updated values within the same sweep. Typically converges faster.\"\"\"\n",
    "    V = {s: 0.0 for s in STATES}\n",
    "    iters = 0\n",
    "    while True:\n",
    "        delta = 0.0\n",
    "        for s in STATES:\n",
    "            if is_terminal(s):\n",
    "                V[s] = 0.0\n",
    "                continue\n",
    "            best = float(\"-inf\")\n",
    "            for a in ACTIONS:\n",
    "                s2, r = step(s, a)\n",
    "                q = r + gamma * V[s2]  # reuse newly updated V[s2] when available\n",
    "                if q > best:\n",
    "                    best = q\n",
    "            delta = max(delta, abs(best - V[s]))\n",
    "            V[s] = best\n",
    "        iters += 1\n",
    "        if delta < theta or iters >= max_iters:\n",
    "            break\n",
    "    return V, iters\n",
    "\n",
    "# ----- Greedy Policy from V -----\n",
    "def greedy_policy(V):\n",
    "    \"\"\"π*(s) = argmax_a [ r(s→s′) + γ V(s′) ]\"\"\"\n",
    "    pi = {}\n",
    "    for s in STATES:\n",
    "        if is_terminal(s):\n",
    "            pi[s] = \"•\"  # terminal marker\n",
    "            continue\n",
    "        best_a, best_q = None, float(\"-inf\")\n",
    "        for a in ACTIONS:\n",
    "            s2, r = step(s, a)\n",
    "            q = r + gamma * V[s2]\n",
    "            if q > best_q:\n",
    "                best_q, best_a = q, a\n",
    "        pi[s] = best_a\n",
    "    return pi\n",
    "\n",
    "# ----- Pretty printing helpers -----\n",
    "def print_value_grid(V, title=\"Value Function\"):\n",
    "    print(f\"\\n{title}\")\n",
    "    for r in range(H):\n",
    "        row = [f\"{V[(r,c)]:7.3f}\" for c in range(W)]\n",
    "        print(\" \".join(row))\n",
    "\n",
    "def print_policy_grid(pi, title=\"Policy (greedy)\"):\n",
    "    arrows = {\"up\":\"↑\", \"down\":\"↓\", \"left\":\"←\", \"right\":\"→\", \"•\":\"•\"}\n",
    "    print(f\"\\n{title}\")\n",
    "    for r in range(H):\n",
    "        row = []\n",
    "        for c in range(W):\n",
    "            s = (r, c)\n",
    "            if s in GREY and s != GOAL:\n",
    "                row.append(\"X\")  # mark grey cells\n",
    "            else:\n",
    "                row.append(arrows.get(pi[s], \"?\"))\n",
    "        print(\" \".join(row))\n",
    "\n",
    "def l1_diff(Va, Vb):\n",
    "    return sum(abs(Va[s] - Vb[s]) for s in STATES)\n",
    "\n",
    "# ----- Run both methods and compare -----\n",
    "t0 = perf_counter()\n",
    "V_sync, it_sync = value_iteration_synchronous(theta=1e-8, max_iters=10000)\n",
    "t1 = perf_counter()\n",
    "\n",
    "V_inp, it_inp = value_iteration_inplace(theta=1e-8, max_iters=10000)\n",
    "t2 = perf_counter()\n",
    "\n",
    "pi_sync = greedy_policy(V_sync)\n",
    "pi_inp  = greedy_policy(V_inp)\n",
    "\n",
    "print_value_grid(V_sync, title=f\"Standard VI: V* (converged in {it_sync} sweeps, {t1 - t0:.4f}s)\")\n",
    "print_policy_grid(pi_sync, title=\"Standard VI: π*\")\n",
    "\n",
    "print_value_grid(V_inp, title=f\"In-Place VI: V* (converged in {it_inp} sweeps, {t2 - t1:.4f}s)\")\n",
    "print_policy_grid(pi_inp, title=\"In-Place VI: π*\")\n",
    "\n",
    "print(\"\\n=== Convergence / Equality Check ===\")\n",
    "print(f\"Standard VI sweeps: {it_sync}, time: {t1 - t0:.4f}s\")\n",
    "print(f\"In-Place  VI sweeps: {it_inp}, time: {t2 - t1:.4f}s\")\n",
    "print(f\"L1 difference between V* (should be ~0): {l1_diff(V_sync, V_inp):.8f}\")\n",
    "\n",
    "# Optional: assert near-equality of value functions\n",
    "# (use a tolerance since floating point can differ slightly)\n",
    "tol = 1e-6\n",
    "if l1_diff(V_sync, V_inp) < tol:\n",
    "    print(\"OK: V* from both methods match within tolerance.\")\n",
    "else:\n",
    "    print(\"Warning: V* mismatch beyond tolerance.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89835bf4",
   "metadata": {},
   "source": [
    "- 5×5 grid: goal gives **+10**, grey cells **−5**, all other steps **−1**.  \n",
    "- Value Iteration repeatedly sets \\(V(s)=\\max_a\\{\\text{reward(next)}+\\gamma V(\\text{next})\\}\\), making squares nearer the goal worth more and penalizing grey/long routes.  \n",
    "- The greedy policy then points along the **shortest safe path** to the goal.  \n",
    "- **In-place** VI converges faster but ends with the **same \\(V^*\\) and \\(\\pi^*\\)** as standard VI.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
