{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07bab7af",
   "metadata": {},
   "source": [
    "# CSCN8020 – Assignment 1 \n",
    "\n",
    "**Course:** Reinforcement Learning Programming (CSCN8020)  \n",
    "**Student:** Jahnavi Pakanati\n",
    "**Student ID:** 9013742\n",
    "\n",
    "This notebook contains:\n",
    "- **Problem 1**: MDP design for a pick-and-place robot  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcdd4aee",
   "metadata": {},
   "source": [
    "## Problem 1 – Pick-and-Place Robot (MDP Design)\n",
    "\n",
    "We model the **pick-and-place** task as a **Markov Decision Process (MDP)**.\n",
    "\n",
    "#### 1) States (S)\n",
    "A state should capture the **minimum information** needed to choose good actions:\n",
    "- Robot arm pose (discretized or continuous), e.g., `(x, y, z)` or `(joint1, joint2, joint3, ...)`\n",
    "- Gripper: `open` / `closed`\n",
    "- Object status: `on_table` / `in_gripper` / `at_target`\n",
    "- (Optional) Velocities for smoothness: `vx, vy, vz`\n",
    "\n",
    "##### 2) Actions (A)\n",
    "Primitive, low-level actions that the policy can choose:\n",
    "- Arm motion: `move_up`, `move_down`, `move_left`, `move_right` (optionally `move_forward`, `move_backward`)\n",
    "- Gripper: `open_gripper`, `close_gripper`\n",
    "\n",
    "##### 3) Rewards (R)\n",
    "Shape the behavior to be **fast** and **smooth**:\n",
    "- `+10` when the object is placed at the target location (`at_target`)\n",
    "- `-1` per time step to discourage unnecessary movement\n",
    "- `-5` penalty for dropping the object or collisions\n",
    "\n",
    "> ***Reasoning:*** This reward encourages successful completion, penalizes wasted motion, and discourages unsafe behaviors. The state and action choices reflect the ***control levers*** the agent has and the **task status** it must track.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242551d1",
   "metadata": {},
   "source": [
    "# Problem 2 — 2×2 Gridworld (Two Value-Iteration Sweeps)\n",
    "\n",
    "**Setup**\n",
    "- **States:** \\(s_1=(0,0),\\ s_2=(0,1),\\ s_3=(1,0),\\ s_4=(1,1)\\)  \n",
    "- **Actions:** up, down, left, right  \n",
    "- **Transitions:** deterministic if valid; **invalid moves keep you in the same state** (\\(s' = s\\))  \n",
    "- **Rewards (per state):** \\(R(s_1)=5,\\ R(s_2)=10,\\ R(s_3)=1,\\ R(s_4)=2\\)  \n",
    "- **Discount:** \\(\\gamma=0.9\\)  \n",
    "- **Update rule (Bellman optimality for this state-reward setting):**  \n",
    "  \\[\n",
    "  V_{k+1}(s)\\ \\leftarrow\\ R(s)\\ +\\ \\gamma\\ \\max_{a\\in A}\\ V_k\\!\\big(s'(s,a)\\big)\n",
    "  \\]\n",
    "\n",
    "---\n",
    "\n",
    "## Iteration 0 (Initialization)\n",
    "All zeros:\n",
    "| State | \\(V_0(s)\\) |\n",
    "|---|---|\n",
    "| \\(s_1\\) | 0 |\n",
    "| \\(s_2\\) | 0 |\n",
    "| \\(s_3\\) | 0 |\n",
    "| \\(s_4\\) | 0 |\n",
    "\n",
    "---\n",
    "\n",
    "## Iteration 1\n",
    "Neighbors were 0 in Iteration 0, so:\n",
    "\\[\n",
    "V_1(s) = R(s) + \\gamma\\cdot 0 = R(s)\n",
    "\\]\n",
    "\n",
    "| State | Calculation | \\(V_1(s)\\) |\n",
    "|---|---|---|\n",
    "| \\(s_1\\) | \\(5 + 0.9\\cdot 0\\) | **5** |\n",
    "| \\(s_2\\) | \\(10 + 0.9\\cdot 0\\) | **10** |\n",
    "| \\(s_3\\) | \\(1 + 0.9\\cdot 0\\) | **1** |\n",
    "| \\(s_4\\) | \\(2 + 0.9\\cdot 0\\) | **2** |\n",
    "\n",
    "---\n",
    "\n",
    "## Iteration 2\n",
    "Look one step ahead using \\(V_1\\). *(Invalid moves allow “stay”.)*\n",
    "\n",
    "- **\\(s_1\\)**: up→\\(s_1\\) (5), left→\\(s_1\\) (5), right→\\(s_2\\) (10), down→\\(s_3\\) (1)  \n",
    "  \\[\n",
    "  V_2(s_1)=5 + 0.9\\cdot \\max(5,5,10,1)=5+0.9\\cdot 10=\\mathbf{14}\n",
    "  \\]\n",
    "- **\\(s_2\\)**: up→\\(s_2\\) (10), right→\\(s_2\\) (10), left→\\(s_1\\) (5), down→\\(s_4\\) (2)  \n",
    "  \\[\n",
    "  V_2(s_2)=10 + 0.9\\cdot \\max(10,10,5,2)=10+0.9\\cdot 10=\\mathbf{19}\n",
    "  \\]\n",
    "- **\\(s_3\\)**: left→\\(s_3\\) (1), down→\\(s_3\\) (1), up→\\(s_1\\) (5), right→\\(s_4\\) (2)  \n",
    "  \\[\n",
    "  V_2(s_3)=1 + 0.9\\cdot \\max(1,1,5,2)=1+0.9\\cdot 5=\\mathbf{5.5}\n",
    "  \\]\n",
    "- **\\(s_4\\)**: right→\\(s_4\\) (2), down→\\(s_4\\) (2), up→\\(s_2\\) (10), left→\\(s_3\\) (1)  \n",
    "  \\[\n",
    "  V_2(s_4)=2 + 0.9\\cdot \\max(2,2,10,1)=2+0.9\\cdot 10=\\mathbf{11}\n",
    "  \\]\n",
    "\n",
    "**Values after Iteration 2**\n",
    "\n",
    "| State | \\(V_2(s)\\) |\n",
    "|---|---|\n",
    "| \\(s_1\\) | **14.0** |\n",
    "| \\(s_2\\) | **19.0** |\n",
    "| \\(s_3\\) | **5.5** |\n",
    "| \\(s_4\\) | **11.0** |\n",
    "\n",
    "---\n",
    "\n",
    "## Greedy Policy after Iteration 2\n",
    "Pick \\(a=\\arg\\max_a \\big(R(s)+\\gamma V_2(s'(s,a))\\big)\\).\n",
    "\n",
    "- \\(s_1\\): best next is \\(s_2\\) → **right**  \n",
    "- \\(s_2\\): best is to **stay at \\(s_2\\)** (via an invalid move like up/right)  \n",
    "- \\(s_3\\): best next is \\(s_1\\) → **up**  \n",
    "- \\(s_4\\): best next is \\(s_2\\) → **up**\n",
    "\n",
    "> **Note:** If exploiting invalid moves to “stay” is **not allowed**, the best *valid* action from \\(s_2\\) is **left to \\(s_1\\)** (since \\(V_2(s_1)=14\\) > \\(V_2(s_4)=11\\)).\n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
